# ğŸ› ï¸ Feature Engineering with Amazon SageMaker Data Wrangler 

## Summary
Feature engineering is the process of using domain knowledge to extract new variables (features) from raw data that make machine learning algorithms work more effectively. Amazon SageMaker Data Wrangler transforms this process through its visual interface, allowing you to create features without writing code. This tutorial demonstrates how Data Wrangler streamlines feature creation, from basic transformations to complex derived features that improve model performance.

Key feature engineering techniques include:

- Feature creation: Deriving new features from existing ones
- Feature transformation: Converting features to more useful formats
- Feature selection: Choosing the most relevant features
- Feature encoding: Converting categorical variables to numerical format

Feature engineering is critical for machine learning success, yet it traditionally consumes 60-70% of data scientists' time. 

### 1. Setup and Dataset Creation ğŸ“

First, let's create a dummy e-commerce dataset:

```python
import pandas as pd
import numpy as np
from datetime import datetime, timedelta

# Set seed for reproducibility âœ¨
np.random.seed(42)

# Generate 1000 sample records ğŸ“
n_samples = 1000

# Create base data ğŸ—„ï¸
data = {
    'customer_id': np.arange(1, n_samples + 1),
    'age': np.random.randint(18, 75, n_samples),
    'gender': np.random.choice(['M', 'F', 'Other'], n_samples),
    'location': np.random.choice(['Urban', 'Suburban', 'Rural'], n_samples),
    'membership_days': np.random.randint(1, 1000, n_samples),
    'previous_purchases': np.random.randint(0, 50, n_samples),
    'last_purchase_amount': np.random.uniform(10, 500, n_samples).round(2),
    'items_viewed': np.random.randint(0, 100, n_samples),
    'cart_abandonment_rate': np.random.uniform(0, 1, n_samples).round(2),
    'purchase_made': np.random.choice([0, 1], n_samples, p=[0.7, 0.3])
}

# Add some missing values 
for col in ['age', 'last_purchase_amount', 'items_viewed']:
    mask = np.random.choice([True, False], n_samples, p=[0.05, 0.95])
    data[col] = np.where(mask, np.nan, data[col])

# Create DataFrame ğŸ”¢
ecommerce_df = pd.DataFrame(data)

# Save to CSV ğŸ’¾
ecommerce_df.to_csv('ecommerce_data.csv', index=False)
print("Dataset created and saved as 'ecommerce_data.csv'")
```

Upload this CSV file to an S3 bucket for use with Data Wrangler: ğŸª£

```python
import boto3
import sagemaker

session = boto3.Session()
region = session.region_name
sagemaker_session = sagemaker.Session()
default_bucket = sagemaker_session.default_bucket()
prefix = 'data-wrangler-tutorial'

s3_client = boto3.client('s3')
s3_client.upload_file(
    'ecommerce_data.csv', 
    default_bucket, 
    f'{prefix}/ecommerce_data.csv'
)

print(f"File uploaded to s3://{default_bucket}/{prefix}/ecommerce_data.csv")
```

![image](https://github.com/user-attachments/assets/665d57b7-ca36-41a5-bd5e-0517438fbf22)

### 2. Launch SageMaker Data Wrangler ğŸš€

1. Open Amazon SageMaker Studio ğŸ–¥ï¸
2. From the Studio launcher, click on "New data flow" or navigate to File > New > Flow [[3]](https://aws.amazon.com/sagemaker/data-wrangler/)

![image](https://github.com/user-attachments/assets/7a1aa2d9-f251-4f1f-88a7-af1717db2796)

### 3. Import the Dataset ğŸ“¥

1. In the Data Wrangler interface, select "Import data" ğŸ“¤
2. Choose "Amazon S3" as the data source ğŸª£
3. Navigate to your bucket and select the ecommerce_data.csv file ğŸ“„
4. Click "Import" to load the dataset into Data Wrangler âœ…

   ![image](https://github.com/user-attachments/assets/a3a20a53-a37d-410d-9792-c511fdfc276d)


### 4. Data Exploration and Analysis ğŸ”

1. In the Data Flow view, click on the dataset node ğŸ“Š
2. Select "Data types" to verify column types are correctly assigned âœ“
3. Use "Data quality and insights" to generate a report on your data: ğŸ“ˆ
   * Check for missing values â“
   * View distribution of features ğŸ“Š
   * Identify correlations between features [[4]](https://aws.amazon.com/blogs/machine-learning/sagemaker-data-wrangler-now-auto-generates-feature-level-visualizations/)

### 5. Feature Engineering Steps ğŸ› ï¸

#### Step 1: Handle Missing Values â“â¡ï¸âœ…

1. Click "+ Add step" on your data node â•
2. Select "Handle missing values" ğŸ”§
3. Choose "Impute" for:
   * age: Use "Mean" imputation ğŸ“Š
   * last_purchase_amount: Use "Median" imputation ğŸ’°
   * items_viewed: Use "Custom" with value 0 ğŸ‘ï¸

     ![image](https://github.com/user-attachments/assets/a747179d-7305-4e9b-b988-7c32db4b4e4d)


#### Step 2: Create Customer Segments ğŸ‘¥

1. Click "+ Add step" again â•
2. Select "Custom formula" âœï¸
3. Enter formula to create spending segments:

   ```
   case(
     last_purchase_amount > 300, 'High Spender',
     last_purchase_amount > 100, 'Medium Spender',
     'Low Spender'
   )
   ```


5. Name the new column spending_segment ğŸ’¸

#### Step 3: One-Hot Encode Categorical Variables ğŸ”„

1. Add another step â•
2. Select "Encode categorical" ğŸ·ï¸
3. Choose "One-hot encode" 0ï¸âƒ£1ï¸âƒ£
4. Select columns: gender, location, and spending_segment ğŸ“‹

   ![image](https://github.com/user-attachments/assets/653340fa-ac81-4f8a-9f18-39bef071abad)


#### Step 4: Create Interaction Features âš¡

1. Add another step â•
2. Select "Custom formula" âœï¸
3. Create an engagement score: `(previous_purchases * 0.4) + (items_viewed * 0.3) + ((1 - cart_abandonment_rate) * 0.3)` ğŸ§®
4. Name the new column engagement_score ğŸ“ˆ

![image](https://github.com/user-attachments/assets/37be3e29-127b-4feb-ae34-8ec02332ba65)

### 6. Analyze Model Quality ğŸ“Š

1. Select "Analysis" on your final transformation node ğŸ”
2. Choose "Quick model" analysis ğŸƒâ€â™‚ï¸
3. Set purchase_made as the target column ğŸ¯
4. Select "Classification" as the model type ğŸŒ²
5. Review feature importance metrics 
This Quick Model found the items_viewed to be the most important feature for predicting purchase_made. The score of 0.14 for the most important feature, suggests  a model where predictive power is distributed across multiple features rather than dominated by a single one. 
   ![image](https://github.com/user-attachments/assets/812db677-bffe-41fd-9a2a-fdd6b1f3f498)

![image](https://github.com/user-attachments/assets/8a8f2f04-fb03-456e-aee9-c363892efe5d)


### 7. Export the Transformed Data ğŸ“¤

1. Click "+ Add destination" on your final node â¡ï¸
2. Choose one of the following options:
   * Export to S3 ğŸª£
   * Export to SageMaker Feature Store ğŸª
   * Export to SageMaker Pipelines ğŸ”„
   * Export to SageMaker Autopilot ğŸ¤– [[6]](https://aws.amazon.com/sagemaker/data-wrangler/)
3. For this tutorial, select "Amazon S3" ğŸª£
4. Specify your S3 location and file format (e.g., CSV or Parquet) ğŸ“„
5. Click "Export" to start the processing job ğŸš€

### 8. Clean Up ğŸ§¹

To avoid incurring unnecessary charges, make sure to:

1. Shut down the SageMaker Data Wrangler application ğŸ›‘
2. Delete any resources created during this tutorial [[7]](https://aws.amazon.com/sagemaker/data-wrangler/)

## Key Benefits Demonstrated âœ¨

* ğŸ§™â€â™‚ï¸ No-code data preparation: Transformed raw data into ML-ready features without writing code
* ğŸ‘ï¸ Visual data exploration: Quickly understood data quality issues and distributions
* âš¡ Efficient feature engineering: Created new features and transformed existing ones in minutes
* ğŸ” Model validation: Used Quick Model to validate feature importance before model building
* ğŸ”„ Streamlined workflow: Prepared data for ML model training in a single interface [[8]](https://aws.amazon.com/sagemaker/data-wrangler/)

